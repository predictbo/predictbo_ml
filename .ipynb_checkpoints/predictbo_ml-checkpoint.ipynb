{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7908974e-1cc2-47fd-ba8f-6a0b7a7f2993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 1.3446 - mae: 1.0381 - val_loss: 0.2001 - val_mae: 0.3634\n",
      "Epoch 2/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1953 - mae: 0.3565 - val_loss: 0.1904 - val_mae: 0.3402\n",
      "Epoch 3/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1464 - mae: 0.2986 - val_loss: 0.1081 - val_mae: 0.2638\n",
      "Epoch 4/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0914 - mae: 0.2398 - val_loss: 0.0679 - val_mae: 0.2092\n",
      "Epoch 5/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0536 - mae: 0.1806 - val_loss: 0.0417 - val_mae: 0.1577\n",
      "Epoch 6/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0338 - mae: 0.1342 - val_loss: 0.0231 - val_mae: 0.1117\n",
      "Epoch 7/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0168 - mae: 0.0891 - val_loss: 0.0194 - val_mae: 0.1012\n",
      "Epoch 8/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0187 - mae: 0.0947 - val_loss: 0.0187 - val_mae: 0.0965\n",
      "Epoch 9/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0221 - mae: 0.0910 - val_loss: 0.0137 - val_mae: 0.0722\n",
      "Epoch 10/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0198 - mae: 0.0769 - val_loss: 0.0146 - val_mae: 0.0713\n",
      "Epoch 11/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0123 - mae: 0.0616 - val_loss: 0.0152 - val_mae: 0.0784\n",
      "Epoch 12/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0119 - mae: 0.0639 - val_loss: 0.0128 - val_mae: 0.0697\n",
      "Epoch 13/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0098 - mae: 0.0542 - val_loss: 0.0143 - val_mae: 0.0751\n",
      "Epoch 14/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0131 - mae: 0.0583 - val_loss: 0.0126 - val_mae: 0.0649\n",
      "Epoch 15/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0125 - mae: 0.0573 - val_loss: 0.0137 - val_mae: 0.0725\n",
      "Epoch 16/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0153 - mae: 0.0609 - val_loss: 0.0127 - val_mae: 0.0626\n",
      "Epoch 17/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0130 - mae: 0.0548 - val_loss: 0.0139 - val_mae: 0.0708\n",
      "Epoch 18/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0123 - mae: 0.0586 - val_loss: 0.0128 - val_mae: 0.0665\n",
      "Epoch 19/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0122 - mae: 0.0579 - val_loss: 0.0126 - val_mae: 0.0690\n",
      "Epoch 19: early stopping\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 1.0770 - mae: 0.9188 - val_loss: 0.1889 - val_mae: 0.3507\n",
      "Epoch 2/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1982 - mae: 0.3629 - val_loss: 0.1214 - val_mae: 0.2772\n",
      "Epoch 3/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1116 - mae: 0.2680 - val_loss: 0.0868 - val_mae: 0.2345\n",
      "Epoch 4/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0795 - mae: 0.2226 - val_loss: 0.0527 - val_mae: 0.1670\n",
      "Epoch 5/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0412 - mae: 0.1534 - val_loss: 0.0333 - val_mae: 0.1239\n",
      "Epoch 6/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0270 - mae: 0.1076 - val_loss: 0.0244 - val_mae: 0.0796\n",
      "Epoch 7/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0186 - mae: 0.0791 - val_loss: 0.0253 - val_mae: 0.0816\n",
      "Epoch 8/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0121 - mae: 0.0615 - val_loss: 0.0212 - val_mae: 0.0637\n",
      "Epoch 9/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0114 - mae: 0.0590 - val_loss: 0.0199 - val_mae: 0.0539\n",
      "Epoch 10/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0112 - mae: 0.0534 - val_loss: 0.0188 - val_mae: 0.0580\n",
      "Epoch 11/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0094 - mae: 0.0533 - val_loss: 0.0187 - val_mae: 0.0638\n",
      "Epoch 12/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0139 - mae: 0.0633 - val_loss: 0.0203 - val_mae: 0.0689\n",
      "Epoch 13/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0127 - mae: 0.0630 - val_loss: 0.0193 - val_mae: 0.0635\n",
      "Epoch 14/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0083 - mae: 0.0571 - val_loss: 0.0193 - val_mae: 0.0558\n",
      "Epoch 15/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0112 - mae: 0.0533 - val_loss: 0.0209 - val_mae: 0.0816\n",
      "Epoch 16/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0163 - mae: 0.0714 - val_loss: 0.0192 - val_mae: 0.0642\n",
      "Epoch 16: early stopping\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 1.0340 - mae: 0.8881 - val_loss: 0.1898 - val_mae: 0.3592\n",
      "Epoch 2/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1594 - mae: 0.3207 - val_loss: 0.1362 - val_mae: 0.2945\n",
      "Epoch 3/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0982 - mae: 0.2500 - val_loss: 0.0976 - val_mae: 0.2375\n",
      "Epoch 4/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0536 - mae: 0.1838 - val_loss: 0.0667 - val_mae: 0.1791\n",
      "Epoch 5/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0398 - mae: 0.1441 - val_loss: 0.0428 - val_mae: 0.1222\n",
      "Epoch 6/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0202 - mae: 0.0974 - val_loss: 0.0306 - val_mae: 0.0993\n",
      "Epoch 7/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0156 - mae: 0.0829 - val_loss: 0.0348 - val_mae: 0.1067\n",
      "Epoch 8/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0136 - mae: 0.0798 - val_loss: 0.0271 - val_mae: 0.0816\n",
      "Epoch 9/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0150 - mae: 0.0723 - val_loss: 0.0311 - val_mae: 0.0752\n",
      "Epoch 10/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0106 - mae: 0.0577 - val_loss: 0.0277 - val_mae: 0.0694\n",
      "Epoch 11/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0099 - mae: 0.0498 - val_loss: 0.0256 - val_mae: 0.0714\n",
      "Epoch 12/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0128 - mae: 0.0618 - val_loss: 0.0254 - val_mae: 0.0688\n",
      "Epoch 13/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0100 - mae: 0.0532 - val_loss: 0.0320 - val_mae: 0.0861\n",
      "Epoch 14/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0150 - mae: 0.0691 - val_loss: 0.0291 - val_mae: 0.0690\n",
      "Epoch 15/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0085 - mae: 0.0473 - val_loss: 0.0248 - val_mae: 0.0620\n",
      "Epoch 16/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0119 - mae: 0.0490 - val_loss: 0.0259 - val_mae: 0.0631\n",
      "Epoch 17/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0060 - mae: 0.0429 - val_loss: 0.0235 - val_mae: 0.0696\n",
      "Epoch 18/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0126 - mae: 0.0557 - val_loss: 0.0269 - val_mae: 0.0672\n",
      "Epoch 19/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0125 - mae: 0.0478 - val_loss: 0.0253 - val_mae: 0.0602\n",
      "Epoch 20/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0133 - mae: 0.0500 - val_loss: 0.0289 - val_mae: 0.0690\n",
      "Epoch 21/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0109 - mae: 0.0499 - val_loss: 0.0290 - val_mae: 0.0770\n",
      "Epoch 22/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0101 - mae: 0.0546 - val_loss: 0.0235 - val_mae: 0.0666\n",
      "Epoch 23/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0086 - mae: 0.0490 - val_loss: 0.0258 - val_mae: 0.0603\n",
      "Epoch 24/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0133 - mae: 0.0508 - val_loss: 0.0286 - val_mae: 0.0775\n",
      "Epoch 25/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0080 - mae: 0.0512 - val_loss: 0.0288 - val_mae: 0.0851\n",
      "Epoch 26/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0081 - mae: 0.0536 - val_loss: 0.0239 - val_mae: 0.0625\n",
      "Epoch 27/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0077 - mae: 0.0456 - val_loss: 0.0289 - val_mae: 0.0991\n",
      "Epoch 27: early stopping\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 1.0838 - mae: 0.9135 - val_loss: 0.1761 - val_mae: 0.3385\n",
      "Epoch 2/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1862 - mae: 0.3472 - val_loss: 0.1607 - val_mae: 0.3264\n",
      "Epoch 3/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1148 - mae: 0.2749 - val_loss: 0.1060 - val_mae: 0.2595\n",
      "Epoch 4/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0771 - mae: 0.2188 - val_loss: 0.0763 - val_mae: 0.2077\n",
      "Epoch 5/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0584 - mae: 0.1830 - val_loss: 0.0567 - val_mae: 0.1742\n",
      "Epoch 6/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0407 - mae: 0.1462 - val_loss: 0.0411 - val_mae: 0.1365\n",
      "Epoch 7/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0299 - mae: 0.1129 - val_loss: 0.0265 - val_mae: 0.0992\n",
      "Epoch 8/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0201 - mae: 0.0844 - val_loss: 0.0206 - val_mae: 0.0778\n",
      "Epoch 9/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0151 - mae: 0.0691 - val_loss: 0.0181 - val_mae: 0.0735\n",
      "Epoch 10/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0127 - mae: 0.0613 - val_loss: 0.0163 - val_mae: 0.0704\n",
      "Epoch 11/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0141 - mae: 0.0625 - val_loss: 0.0139 - val_mae: 0.0618\n",
      "Epoch 12/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0143 - mae: 0.0555 - val_loss: 0.0126 - val_mae: 0.0559\n",
      "Epoch 13/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0093 - mae: 0.0500 - val_loss: 0.0129 - val_mae: 0.0633\n",
      "Epoch 14/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0127 - mae: 0.0561 - val_loss: 0.0125 - val_mae: 0.0580\n",
      "Epoch 15/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0174 - mae: 0.0614 - val_loss: 0.0115 - val_mae: 0.0568\n",
      "Epoch 16/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0140 - mae: 0.0560 - val_loss: 0.0123 - val_mae: 0.0614\n",
      "Epoch 17/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0169 - mae: 0.0543 - val_loss: 0.0117 - val_mae: 0.0643\n",
      "Epoch 18/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0184 - mae: 0.0718 - val_loss: 0.0127 - val_mae: 0.0686\n",
      "Epoch 19/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0107 - mae: 0.0573 - val_loss: 0.0142 - val_mae: 0.0742\n",
      "Epoch 20/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0112 - mae: 0.0554 - val_loss: 0.0125 - val_mae: 0.0685\n",
      "Epoch 20: early stopping\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 1.1654 - mae: 0.9539 - val_loss: 0.1784 - val_mae: 0.3543\n",
      "Epoch 2/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2096 - mae: 0.3813 - val_loss: 0.1183 - val_mae: 0.2786\n",
      "Epoch 3/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1181 - mae: 0.2779 - val_loss: 0.0682 - val_mae: 0.2146\n",
      "Epoch 4/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0850 - mae: 0.2364 - val_loss: 0.0390 - val_mae: 0.1632\n",
      "Epoch 5/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0479 - mae: 0.1720 - val_loss: 0.0166 - val_mae: 0.1031\n",
      "Epoch 6/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0300 - mae: 0.1221 - val_loss: 0.0089 - val_mae: 0.0721\n",
      "Epoch 7/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0217 - mae: 0.0914 - val_loss: 0.0082 - val_mae: 0.0661\n",
      "Epoch 8/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0182 - mae: 0.0751 - val_loss: 0.0056 - val_mae: 0.0554\n",
      "Epoch 9/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0182 - mae: 0.0696 - val_loss: 0.0051 - val_mae: 0.0536\n",
      "Epoch 10/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0205 - mae: 0.0704 - val_loss: 0.0053 - val_mae: 0.0529\n",
      "Epoch 11/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0243 - mae: 0.0731 - val_loss: 0.0051 - val_mae: 0.0548\n",
      "Epoch 12/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0184 - mae: 0.0701 - val_loss: 0.0039 - val_mae: 0.0452\n",
      "Epoch 13/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0142 - mae: 0.0588 - val_loss: 0.0037 - val_mae: 0.0450\n",
      "Epoch 14/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0197 - mae: 0.0678 - val_loss: 0.0038 - val_mae: 0.0447\n",
      "Epoch 15/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0178 - mae: 0.0644 - val_loss: 0.0033 - val_mae: 0.0426\n",
      "Epoch 16/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0147 - mae: 0.0605 - val_loss: 0.0040 - val_mae: 0.0486\n",
      "Epoch 17/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0186 - mae: 0.0664 - val_loss: 0.0052 - val_mae: 0.0516\n",
      "Epoch 18/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0138 - mae: 0.0564 - val_loss: 0.0050 - val_mae: 0.0547\n",
      "Epoch 19/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0175 - mae: 0.0744 - val_loss: 0.0043 - val_mae: 0.0461\n",
      "Epoch 20/1000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0157 - mae: 0.0581 - val_loss: 0.0060 - val_mae: 0.0568\n",
      "Epoch 20: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline and model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import KBinsDiscretizer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib\n",
    "from transformers import Discretizer  # Import the Discretizer class\n",
    "\n",
    "# Load datasets\n",
    "experimental_data = pd.read_csv(\"EXPERIMENTAL_DATA_RENAMED.csv\")\n",
    "rashidi_data = pd.read_csv(\"Rashidi.csv\")\n",
    "\n",
    "# Define features and target\n",
    "features = [\n",
    "    \"Solution Gas Oil Ratio, (SCF/STB)\", \n",
    "    \"Temperature, (F)\", \n",
    "    \"API, (-)\", \n",
    "    \"Bubble Point Pressure, (Psi)\"\n",
    "]\n",
    "target = \"Oil Formation Volume Factor, (bbl/STB)\"\n",
    "\n",
    "# Prepare datasets\n",
    "experimental_data = experimental_data[features + [\"Bo\"]].rename(columns={\"Bo\": target}).dropna()\n",
    "rashidi_data = rashidi_data[features + [target]].dropna()\n",
    "\n",
    "# Combine datasets\n",
    "combined_data = pd.concat([experimental_data, rashidi_data], ignore_index=True)\n",
    "\n",
    "# Split features and target\n",
    "X = combined_data[features].values\n",
    "y = combined_data[target].values\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('discretizer', Discretizer(feature_indices=[0, 1, 3])),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# K-Fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare a function to create the model\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        layers.Input(shape=(X.shape[1],)),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-6)),\n",
    "        layers.Dense(137, activation='relu', kernel_regularizer=regularizers.l2(1e-6)),\n",
    "        layers.Dense(163, activation='relu', kernel_regularizer=regularizers.l2(1e-6)),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Train the model using K-Fold\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Preprocess the data\n",
    "    X_train_transformed = pipeline.fit_transform(X_train, y_train)\n",
    "    X_val_transformed = pipeline.transform(X_val)\n",
    "\n",
    "    # Create and train the model\n",
    "    model = create_model()\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_transformed, y_train,\n",
    "        validation_data=(X_val_transformed, y_val),\n",
    "        epochs=1000,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Save the preprocessing pipeline\n",
    "joblib.dump(pipeline, 'preprocessing_pipeline.pkl')\n",
    "\n",
    "# Save the trained Keras model\n",
    "model.save('trained_model.h5')\n",
    "\n",
    "print(\"Pipeline and model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faae530c-df9e-48c2-8001-bec9c408bf82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
